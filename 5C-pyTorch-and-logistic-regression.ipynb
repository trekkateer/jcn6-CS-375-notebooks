{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trekkateer/jcn6-CS-375-notebooks/blob/main/pyTorch-and-logistic-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg9eiRYUXH7j"
      },
      "source": [
        "# From Linear Regression in NumPy to Logistic Regression in PyTorch\n",
        "\n",
        "## Goals\n",
        "\n",
        "- Compare implementing regression in NumPy vs PyTorch\n",
        "- Extend regression to classification by adding softmax and cross-entropy\n",
        "- Practice working with PyTorch's basic APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko82LxlfXH7l"
      },
      "source": [
        "### Course Objectives Addressed\n",
        "\n",
        "This notebook addresses the following CS 375 course objectives:\n",
        "\n",
        "- TM-LinearLayers: \"I can implement linear (fully-connected) layers using efficient parallel code.\"\n",
        "- TM-Softmax: \"I can implement softmax and explain its role in classification networks.\"\n",
        "- OG-LossFunctions: \"I can select and compute appropriate loss functions for regression and classification tasks.\"\n",
        "- OG-ProblemFraming-Supervised: \"I can frame a problem as a supervised learning task with appropriate inputs, targets, and loss function.\"\n",
        "\n",
        "It will also help set you up to make progress towards the following objectives in future labs:\n",
        "\n",
        "- TM-Implement-TrainingLoop: \"I can implement a basic training loop in PyTorch.\"\n",
        "- TM-Autograd: \"I can explain the purpose of automatic differentiation and identify how it is used in PyTorch code.\"\n",
        "\n",
        "This notebook ties together linear layers, softmax, and cross-entropy loss into a complete classification pipeline, bridging from NumPy to PyTorch and from regression to classification â€” a foundation for the multi-layer networks you'll build next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_LMhV8gXH7m"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmrmgcrFXH7m"
      },
      "source": [
        "Let's import necessary modules: *pandas* and NumPy for data wrangling, Matplotlib for plotting, and some sklearn utilities. Now we'll also be importing PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BLzO6SZLXH7m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.optimize\n",
        "\n",
        "# This utility function (inspired by https://github.com/HIPS/autograd/blob/master/autograd/misc/flatten.py)\n",
        "# is useful for optimizing using scipy.optimize.minimize.\n",
        "def flatten(arrs):\n",
        "    \"\"\"Return a 1D array containing the elements of the input arrays,\n",
        "    and an unflatten function that takes a flattened array and returns\n",
        "    the original arrays.\n",
        "    \"\"\"\n",
        "    arrs = [np.asarray(arr) for arr in arrs]\n",
        "    shapes = [arr.shape for arr in arrs]\n",
        "    flat = np.concatenate([arr.ravel() for arr in arrs])\n",
        "    start_indices = np.cumsum([0] + [arr.size for arr in arrs])\n",
        "    def unflatten(params):\n",
        "        return [params[start_indices[i]:start_indices[i+1]].reshape(shape)\n",
        "                for i, shape in enumerate(shapes)]\n",
        "    return flat, unflatten\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA8iogl0XH7n"
      },
      "source": [
        "We'll load the data as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PWEx4f38XH7n"
      },
      "outputs": [],
      "source": [
        "ames = pd.read_csv('https://github.com/kcarnold/AmesHousing/blob/master/data/ames.csv.gz?raw=true', compression=\"gzip\")\n",
        "ames['price'] = ames[\"Sale_Price\"] / 100_000 # Make `price` be in units of $100k, to be easier to interpret.\n",
        "\n",
        "# Create price categories (0, 1, or 2) for the classification task\n",
        "n_classes = 3\n",
        "ames['price_rank'] = ames['price'].rank(pct=True)\n",
        "ames['price_bin'] = pd.cut(ames['price_rank'], bins=n_classes, labels=False)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_names = ['Longitude', 'Latitude']\n",
        "X = ames[feature_names].values\n",
        "y = ames['price_bin'].values\n",
        "\n",
        "# standardize the features, to make the optimization more stable\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X = (X - X_mean) / X_std\n",
        "\n",
        "# Split data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKP8wxB6XH7o"
      },
      "source": [
        "### Basic EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrnpylPXH7o"
      },
      "source": [
        "Histogram of target values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "-U2ogPOjXH7o",
        "outputId": "a85ebdc8-44c0-4081-88e9-bbf19bd11517"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price_bin\n",
              "0    981\n",
              "1    980\n",
              "2    969\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>price_bin</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist(y_train);\n",
        "ames['price_bin'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VdCrgBaXH7p"
      },
      "source": [
        "Check shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nef1-jeBXH7p",
        "outputId": "63c8c3a2-1764-4aac-f362-0898d4d8a914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: X.shape=(2344, 2), y.shape=(2344,)\n",
            "valid: X.shape=(586, 2), y.shape=(586,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"train: X.shape={X_train.shape}, y.shape={y_train.shape}\")\n",
        "print(f\"valid: X.shape={X_valid.shape}, y.shape={y_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPQvDUdpXH7p"
      },
      "source": [
        "Histogram of input feature values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "TzCN9V-6XH7p",
        "outputId": "c25cdc72-d45b-429b-d48e-427cc69073f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist(X_train.flatten(), bins=30);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6TmzTCXH7p"
      },
      "source": [
        "## Part 1: Classification the wrong way (using linear regression)\n",
        "\n",
        "Let's start by treating this as a regression problem - predicting the class number (0, 1, or 2) directly. We'll fit a linear regression model to the target as if it were a continuous variable. **This isn't the right way to do it**, but it will help us build up to the correct way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvfBu8y0XH7p"
      },
      "source": [
        "### 1.A Using NumPy\n",
        "\n",
        "You've seen the code below already. Fill in the blanks to complete the linear regression model in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FEp3CzQXH7p",
        "outputId": "249ad9b6-2cd6-4476-dd33-55a07a74538e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 1.33\n",
            "Final loss: 0.56\n",
            "Fitted weights: [-0.26364467  0.17475394]\n",
            "Fitted bias: 0.9886359247748417\n"
          ]
        }
      ],
      "source": [
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "n_samples, n_features = X_train.shape\n",
        "\n",
        "initial_weights = np.random.standard_normal(size=n_features)\n",
        "initial_bias = np.random.standard_normal()\n",
        "\n",
        "def linear_forward(X, weights, bias):\n",
        "    return X @ weights + bias\n",
        "\n",
        "def compute_mse_loss(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "def loss_given_params(params, X, y_true):\n",
        "    weights, bias = unpack_params(params)\n",
        "    y_pred = linear_forward(X, weights, bias)\n",
        "    return compute_mse_loss(y_true, y_pred)\n",
        "\n",
        "initial_params, unpack_params = flatten([initial_weights, initial_bias])\n",
        "initial_loss = loss_given_params(initial_params, X_train, y_train)\n",
        "optimization_result = scipy.optimize.minimize(loss_given_params, initial_params,\n",
        "                                              args=(X_train, y_train))\n",
        "fitted_weights_np, fitted_bias_np = unpack_params(optimization_result.x)\n",
        "print(f\"Initial loss: {initial_loss:.2f}\")\n",
        "print(f\"Final loss: {optimization_result.fun:.2f}\")\n",
        "print(f\"Fitted weights: {fitted_weights_np}\")\n",
        "print(f\"Fitted bias: {fitted_bias_np}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8JnELtnXH7p"
      },
      "source": [
        "Let's evaluate the \"accuracy\" of this model. We'll round the predictions to the nearest integer to get the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA5ctz7JXH7q",
        "outputId": "e10aca66-5e67-4e82-a238-29971ec2459e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y_pred_valid = linear_forward(X_valid, fitted_weights_np, fitted_bias_np).round()\n",
        "y_pred_valid[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqQRCgvKXH7q",
        "outputId": "90f18aa1-c98a-4421-dc87-236552daadbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for linear regression 'classifier': 0.31\n"
          ]
        }
      ],
      "source": [
        "accuracy = (y_pred_valid == y_valid).mean()\n",
        "print(f\"Validation accuracy for linear regression 'classifier': {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkuBM5LDXH7q"
      },
      "source": [
        "### 1.B: Using PyTorch\n",
        "\n",
        "Now we'll implement the same (wrong) model in PyTorch.\n",
        "\n",
        "We'll build this together step by step. Notice that we're still doing this \"the wrong way\", as regression, not classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcAA510nXH7q",
        "outputId": "8e799359-d311-477f-8228-eedda98ba669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00, Loss: 1.6177\n",
            "Epoch 10, Loss: 0.6660\n",
            "Epoch 20, Loss: 0.6551\n",
            "Epoch 30, Loss: 0.6549\n",
            "Epoch 40, Loss: 0.6549\n",
            "Epoch 50, Loss: 0.6549\n",
            "Epoch 60, Loss: 0.6549\n",
            "Epoch 70, Loss: 0.6549\n",
            "Epoch 80, Loss: 0.6549\n",
            "Epoch 90, Loss: 0.6549\n"
          ]
        }
      ],
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_valid_torch = torch.tensor(X_valid, dtype=torch.float32)\n",
        "# Note: still treating as regression\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_valid_torch = torch.tensor(y_valid, dtype=torch.float32)\n",
        "\n",
        "# Let's start with just learning bias terms\n",
        "class SimpleLinear(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bias.expand(len(x))\n",
        "\n",
        "# TODO (together): Add weights to make it a proper linear model\n",
        "\n",
        "# Training loop (we'll talk next week about how this works)\n",
        "model = SimpleLinear(n_features)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train_torch)\n",
        "    loss = F.mse_loss(y_pred, y_train_torch)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch:02d}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p1ne1QKXH7q"
      },
      "source": [
        "## Part 2: Converting to Classification\n",
        "\n",
        "Now we're going to switch to doing it the right way, as a classification problem. We'll use the softmax function to convert the model's outputs into probabilities, and the cross-entropy loss to train the model to be least surprised by the true class.\n",
        "\n",
        "Again we'll start off by implementing this in NumPy, and then extend it to PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV0ikmCXXH7q"
      },
      "source": [
        "### 2.A Using NumPy (we'll do this together)\n",
        "\n",
        "Let's modify our NumPy implementation to:\n",
        "\n",
        "1. Output one number per class (logits)\n",
        "2. Convert logits to probabilities using softmax\n",
        "3. Use cross-entropy loss instead of MSE\n",
        "\n",
        "We'll start by using one-hot encoding for the outputs, which will make it easy to implement the cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKa6af2GXH7q",
        "outputId": "23ed6d41-3ef9-4647-b903-211250ba3154"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# np.eye is the identity matrix; each row is a one-hot vector.\n",
        "# this code pulls out the one-hot vectors corresponding to the target values\n",
        "y_train_onehot = np.eye(n_classes)[y_train]\n",
        "y_valid_onehot = np.eye(n_classes)[y_valid]\n",
        "\n",
        "y_train_onehot[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCXlkBvwXH7q"
      },
      "source": [
        "Now we'll implement softmax and cross-entropy together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zL_dumaHXH7r"
      },
      "outputs": [],
      "source": [
        "def softmax(logits):\n",
        "    \"\"\"Convert logits to probabilities.\"\"\"\n",
        "    num_samples, num_classes = logits.shape\n",
        "\n",
        "    max_logit = logits.max(axis=1, keepdims=True)\n",
        "\n",
        "    exp_logits = logits.exp()\n",
        "    assert exp_logits.shape == (num_samples, num_classes)\n",
        "\n",
        "    sum_exp_logits = exp_logits.sum(axis=1, keepdims=True)\n",
        "    assert sum_exp_logits.shape == (num_samples, 1)\n",
        "    return exp_logits/sum_exp_logits\n",
        "\n",
        "def cross_entropy_loss(y_true_onehot, probs):\n",
        "    \"\"\"Compute cross entropy loss.\"\"\"\n",
        "    # Strategy:\n",
        "    # - Compute log probabilities\n",
        "    # - Multiply by one-hot vectors and sum, to extract the log probabilities of the true classes\n",
        "    # - Take the negative to get the loss\n",
        "    # - Average the loss over the samples\n",
        "    #\n",
        "    log_probs = np.log(probs + 1e-6) # add a small value to avoid log(0)\n",
        "    loss_per_sample = -np.sum(y_true_onehot * log_probs, axis=1)\n",
        "    assert loss_per_sample.shape == (len(y_true_onehot),)\n",
        "    return loss_per_sample.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB-mmrNfXH7r",
        "outputId": "0962c614-2794-4e9c-e884-885f2bb2629a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial loss: 1.93\n",
            "Final loss: 1.02\n",
            "Fitted weights: [[ 0.7753546   0.39962183 -0.16883813]\n",
            " [ 0.01584829  0.34061106  0.69827992]]\n",
            "Fitted bias: [0.61377123 0.75167041 0.51173135]\n"
          ]
        }
      ],
      "source": [
        "# copy and paste the NumPy code block here and we'll edit.\n",
        "# your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OBy153HXH7r"
      },
      "source": [
        "Now let's compute the accuracy again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Maw3WgXH7r",
        "outputId": "1c2330d2-4fa2-4fb0-c1c1-8d023d2379ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 2, 0, 2, 0, 1, 2, 2])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_valid_logits = linear_forward(X_valid, fitted_weights_np, fitted_bias_np)\n",
        "y_pred_valid = y_pred_valid_logits.argmax(axis=1)\n",
        "y_pred_valid[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMW1MBpSXH7r",
        "outputId": "cbc5b7aa-2126-49e5-e4fb-afbf9f04f11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy for logistic regression: 0.52\n"
          ]
        }
      ],
      "source": [
        "accuracy = (y_pred_valid == y_valid).mean()\n",
        "print(f\"Validation accuracy for logistic regression: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br6nb8iFXH7r"
      },
      "source": [
        "### 2.B Using PyTorch\n",
        "\n",
        "We'll first walk through the PyTorch primitives together, then you'll modify the PyTorch code from Part 1 to use these primitives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2N4mFtWXH7r"
      },
      "outputs": [],
      "source": [
        "y_train_longtensor = torch.tensor(y_train)\n",
        "y_valid_longtensor = torch.tensor(y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rNNyAkIXH7r"
      },
      "source": [
        "#### Setting up the linear layer\n",
        "\n",
        "Fill in the blanks below with the correct number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVrbEQigXH7r",
        "outputId": "1e20e8b7-0080-48f6-d7d4-d441bfbd93b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2344, 3])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_layer = nn.Linear(in_features=..., out_features=..., bias=True)\n",
        "logits = linear_layer(X_train_torch)\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tM1ORszXH7r"
      },
      "source": [
        "#### Softmax\n",
        "\n",
        "PyTorch has builtin functionality for softmax. We can use it like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djsEQEyBXH7r",
        "outputId": "81ad3a7a-09f5-4a6d-b37a-7fb30fd8579f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2344, 3])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs = logits.softmax(axis=1)\n",
        "probs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTqal_wGXH7s"
      },
      "source": [
        "Let's check that the probabilities sum to 1 for each row. Think about what axis you should use for the sum. *General sum rule: whatever axis we sum over will be the axis that disappears.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-Z2Oq89XH7s",
        "outputId": "536c31a3-804f-4829-aae4-1dc439bc7865"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2344])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs_sums = probs.sum(axis=...)\n",
        "probs_sums.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BioXjIEIXH7w",
        "outputId": "574c3183-440d-4326-9ff6-cf7136934f16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs_sums[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a0pnyVkXH7w"
      },
      "source": [
        "#### Cross-entropy loss\n",
        "\n",
        "There are a few different ways we could implement a categorical cross-entropy loss in PyTorch; notice that they give the same output. The `F.cross_entropy` approach is most efficient and numerically stable because **it combines the softmax operation, one-hot encoding, and the cross-entropy loss** into a single step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvCA6UtqXH7w",
        "outputId": "54ed5988-4022-4507-8081-12f25b346f40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.1906, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Direct approach, analogous to the NumPy code above\n",
        "y_train_onehot_torch = torch.tensor(y_train_onehot, dtype=torch.int64)\n",
        "log_probs = probs.log()\n",
        "-(y_train_onehot_torch * log_probs).sum(axis=1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqXJrMoAXH7w",
        "outputId": "0d06bbe4-70e8-4743-a9a2-abc3b64e88c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.1906, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cross-entropy loss function in PyTorch uses logits (without softmax) and class indices (without one-hot)\n",
        "F.cross_entropy(logits, y_train_longtensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EITJi8vGXH7w"
      },
      "source": [
        "<details>\n",
        "  <summary>Other ways to compute cross-entropy loss</summary>\n",
        "\n",
        "```python\n",
        " # see also F.one_hot if you're curious.\n",
        "F.nll_loss(log_probs, y_train_longtensor)\n",
        "F.nll_loss(logits.log_softmax(axis=1), y_train_longtensor)\n",
        "\n",
        "# using the \"object-oriented\" interface\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn(logits, y_train_longtensor)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlccdzjJXH7x"
      },
      "source": [
        "#### Full PyTorch Implementation\n",
        "\n",
        "**Your turn**: Copy and paste your PyTorch linear regression code here and make it into a logistic regression by modifying it to use softmax and cross-entropy loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js-HPb_eXH7x",
        "outputId": "dacb977f-bbb4-44a3-9857-95a261ffaa88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00, Loss: 1.3116\n",
            "Epoch 10, Loss: 1.1680\n",
            "Epoch 20, Loss: 1.0995\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30, Loss: 1.0654\n",
            "Epoch 40, Loss: 1.0468\n",
            "Epoch 50, Loss: 1.0359\n",
            "Epoch 60, Loss: 1.0292\n",
            "Epoch 70, Loss: 1.0249\n",
            "Epoch 80, Loss: 1.0220\n",
            "Epoch 90, Loss: 1.0202\n"
          ]
        }
      ],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKa_FoDoXH7x"
      },
      "source": [
        "### Looking ahead: a multi-layer network\n",
        "\n",
        "Just as a preview for where we're going next week:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "774ma88zXH7x",
        "outputId": "e014eb0f-4c27-4dd7-e9bf-87e3506941da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000, Loss: 1.1152\n",
            "Epoch 010, Loss: 1.0143\n",
            "Epoch 020, Loss: 0.9913\n",
            "Epoch 030, Loss: 0.9761\n",
            "Epoch 040, Loss: 0.9638\n",
            "Epoch 050, Loss: 0.9535\n",
            "Epoch 060, Loss: 0.9446\n",
            "Epoch 070, Loss: 0.9369\n",
            "Epoch 080, Loss: 0.9301\n",
            "Epoch 090, Loss: 0.9240\n",
            "Epoch 100, Loss: 0.9187\n",
            "Epoch 110, Loss: 0.9141\n",
            "Epoch 120, Loss: 0.9099\n",
            "Epoch 130, Loss: 0.9063\n",
            "Epoch 140, Loss: 0.9030\n",
            "Epoch 150, Loss: 0.9001\n",
            "Epoch 160, Loss: 0.8974\n",
            "Epoch 170, Loss: 0.8951\n",
            "Epoch 180, Loss: 0.8929\n",
            "Epoch 190, Loss: 0.8909\n",
            "Epoch 200, Loss: 0.8891\n",
            "Epoch 210, Loss: 0.8875\n",
            "Epoch 220, Loss: 0.8859\n",
            "Epoch 230, Loss: 0.8845\n",
            "Epoch 240, Loss: 0.8832\n",
            "Epoch 250, Loss: 0.8819\n",
            "Epoch 260, Loss: 0.8807\n",
            "Epoch 270, Loss: 0.8795\n",
            "Epoch 280, Loss: 0.8785\n",
            "Epoch 290, Loss: 0.8774\n",
            "Epoch 300, Loss: 0.8765\n",
            "Epoch 310, Loss: 0.8755\n",
            "Epoch 320, Loss: 0.8746\n",
            "Epoch 330, Loss: 0.8737\n",
            "Epoch 340, Loss: 0.8728\n",
            "Epoch 350, Loss: 0.8720\n",
            "Epoch 360, Loss: 0.8712\n",
            "Epoch 370, Loss: 0.8704\n",
            "Epoch 380, Loss: 0.8696\n",
            "Epoch 390, Loss: 0.8688\n",
            "Epoch 400, Loss: 0.8681\n",
            "Epoch 410, Loss: 0.8674\n",
            "Epoch 420, Loss: 0.8667\n",
            "Epoch 430, Loss: 0.8660\n",
            "Epoch 440, Loss: 0.8653\n",
            "Epoch 450, Loss: 0.8646\n",
            "Epoch 460, Loss: 0.8640\n",
            "Epoch 470, Loss: 0.8633\n",
            "Epoch 480, Loss: 0.8627\n",
            "Epoch 490, Loss: 0.8621\n",
            "Epoch 500, Loss: 0.8615\n",
            "Epoch 510, Loss: 0.8609\n",
            "Epoch 520, Loss: 0.8603\n",
            "Epoch 530, Loss: 0.8598\n",
            "Epoch 540, Loss: 0.8592\n",
            "Epoch 550, Loss: 0.8587\n",
            "Epoch 560, Loss: 0.8581\n",
            "Epoch 570, Loss: 0.8576\n",
            "Epoch 580, Loss: 0.8570\n",
            "Epoch 590, Loss: 0.8565\n",
            "Epoch 600, Loss: 0.8560\n",
            "Epoch 610, Loss: 0.8555\n",
            "Epoch 620, Loss: 0.8550\n",
            "Epoch 630, Loss: 0.8545\n",
            "Epoch 640, Loss: 0.8540\n",
            "Epoch 650, Loss: 0.8535\n",
            "Epoch 660, Loss: 0.8530\n",
            "Epoch 670, Loss: 0.8526\n",
            "Epoch 680, Loss: 0.8521\n",
            "Epoch 690, Loss: 0.8517\n",
            "Epoch 700, Loss: 0.8512\n",
            "Epoch 710, Loss: 0.8508\n",
            "Epoch 720, Loss: 0.8503\n",
            "Epoch 730, Loss: 0.8499\n",
            "Epoch 740, Loss: 0.8495\n",
            "Epoch 750, Loss: 0.8491\n",
            "Epoch 760, Loss: 0.8487\n",
            "Epoch 770, Loss: 0.8483\n",
            "Epoch 780, Loss: 0.8479\n",
            "Epoch 790, Loss: 0.8475\n",
            "Epoch 800, Loss: 0.8471\n",
            "Epoch 810, Loss: 0.8467\n",
            "Epoch 820, Loss: 0.8463\n",
            "Epoch 830, Loss: 0.8459\n",
            "Epoch 840, Loss: 0.8455\n",
            "Epoch 850, Loss: 0.8451\n",
            "Epoch 860, Loss: 0.8448\n",
            "Epoch 870, Loss: 0.8444\n",
            "Epoch 880, Loss: 0.8441\n",
            "Epoch 890, Loss: 0.8437\n",
            "Epoch 900, Loss: 0.8434\n",
            "Epoch 910, Loss: 0.8430\n",
            "Epoch 920, Loss: 0.8427\n",
            "Epoch 930, Loss: 0.8423\n",
            "Epoch 940, Loss: 0.8420\n",
            "Epoch 950, Loss: 0.8417\n",
            "Epoch 960, Loss: 0.8413\n",
            "Epoch 970, Loss: 0.8410\n",
            "Epoch 980, Loss: 0.8407\n",
            "Epoch 990, Loss: 0.8404\n",
            "Validation accuracy: 0.61\n"
          ]
        }
      ],
      "source": [
        "n_hidden = 100\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_features=n_features, out_features=n_hidden, bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(in_features=n_hidden, out_features=n_classes, bias=True),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "n_epochs = 1000\n",
        "for epoch in range(n_epochs):\n",
        "    # Forward pass\n",
        "    logits = model(X_train_torch)\n",
        "    loss = F.cross_entropy(logits, y_train_longtensor)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate on validation set\n",
        "logits_valid = model(X_valid_torch)\n",
        "y_pred_valid = logits_valid.argmax(dim=1)\n",
        "accuracy = (y_pred_valid == y_valid_longtensor).float().mean()\n",
        "print(f'Validation accuracy: {accuracy.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72T14YkuXH7y"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "Come back to these questions after you've finished the lab.\n",
        "\n",
        "1. Compare the NumPy and PyTorch implementations. What are the main differences you notice? What are the benefits of PyTorch?\n",
        "\n",
        "2. We came up with 4 weight matrices in the main part of this exercise (linear regression NumPy and PyTorch, logistic regression NumPy and PyTorch). Which of these matrices had the same shapes? Which ones had the same values? Why?\n",
        "\n",
        "3. Which of these two models (linear regression vs logistic regression) does better on this task? (Which number would we use to compare them: loss, accuracy, or something else? Why?)\n",
        "\n",
        "4. Suppose a job interviewer asks you to describe the similarities and differences between linear regression and logistic regression. What would you say? (Hint: discuss how each model makes a prediction, what kinds of patterns in the data they can use, how you measure training progress, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1hpbYKXH7y"
      },
      "source": [
        "*your thoughtful answers here*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "name": "u04n3-logreg-pytorch",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
