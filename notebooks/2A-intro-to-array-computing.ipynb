{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5f5fe490",
      "metadata": {
        "id": "5f5fe490"
      },
      "source": [
        "# PyTorch Warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269c6424",
      "metadata": {
        "id": "269c6424"
      },
      "source": [
        "[PyTorch](https://pytorch.org/) is the open-source machine learning framework that we'll be using in this class. It has a wide range of functionality; for now we'll just get started with some of its very basic array-processing functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed00e346",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:52.427258Z",
          "iopub.status.busy": "2023-01-19T16:24:52.426994Z",
          "iopub.status.idle": "2023-01-19T16:24:56.731553Z",
          "shell.execute_reply": "2023-01-19T16:24:56.730621Z"
        },
        "id": "ed00e346"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c85264",
      "metadata": {
        "id": "e6c85264"
      },
      "source": [
        "## Dot Products\n",
        "\n",
        "The most common basic primitive in a neural network is a *linear* layer (you'll sometimes see it called a \"Dense\" layer). These are where almost all of the parameters go in a network. (Some architectures use a variant called a *convolutional* layer.) At its core, a linear layer does a bunch of *dot product*s between its *input* vector and its (learned) *weight* vectors.\n",
        "\n",
        "A few intuitions to understand what a dot product is:\n",
        "\n",
        "1. It measures *similarity*, in the sense of *alignment*. The following statements loosely capture it:\n",
        "    - \"How much does the input look like *this*?\"\n",
        "    - \"How big is the input in *this* direction?\"\n",
        "    - \"How aligned is the input with this direction?\"\n",
        "    - \"What's the cosine of the angle between the input vector and this vector?\"\n",
        "2. A bunch of dot products all together (like in a Linear layer) *rotates and stretches* the input space, like moving a camera around a scene.\n",
        "3. It's how a multiple linear regression computes its output: a weighted mixture of each part of its input.\n",
        "\n",
        "Recall that we can make a line by an expression like `y = w*x + b`. (Some of you may remember *mx+b* , but we'll use *w* for the *weight(s)* instead.)\n",
        "\n",
        "That's a multiplication followed by a sum. We can extend that to lots of *x*'s, each of which needs a corresponding *w*:\n",
        "\n",
        "`y = w1*x1 + w2*x2 + ... + wN*xN + b`\n",
        "\n",
        "For simplicity, let's ignore the `b` (\"bias\") for now (we'll bring it back later). So we're left with\n",
        "\n",
        "`y = w1*x1 + w2*x2 + ... + wN*xN`\n",
        "\n",
        "that is, multiply each number in `w` by its corresponding number in `x` and add up the results: `sum(w[i] * x[i] for i in range(N))`. Or, in mathematical notation: $\\sum_{i=1}^{N} w_i x_i.$\n",
        "\n",
        "The result is called a *dot product*, and is one of the fundamental operations in linear algebra. At this point you don't need to understand all the linear algebra part of this, we're just implementing a common calculation.\n",
        "\n",
        "Let's do that in pure Python, and then in PyTorch. To start, let's make an array for the weights (called `w`) and an array for the inputs (called `x`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6d290d34",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.735697Z",
          "iopub.status.busy": "2023-01-19T16:24:56.735286Z",
          "iopub.status.idle": "2023-01-19T16:24:56.780008Z",
          "shell.execute_reply": "2023-01-19T16:24:56.779490Z"
        },
        "id": "6d290d34",
        "outputId": "3eaca0a7-8d9e-45c9-f60f-67da1a70d6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2., -1.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "w = torch.tensor([2.0, -1.0])\n",
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2b9487b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.782792Z",
          "iopub.status.busy": "2023-01-19T16:24:56.782584Z",
          "iopub.status.idle": "2023-01-19T16:24:56.787904Z",
          "shell.execute_reply": "2023-01-19T16:24:56.787450Z"
        },
        "id": "2b9487b8",
        "outputId": "a69c355e-adf9-42b3-b241-74e2dd85e045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5000, -3.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x = torch.tensor([1.5, -3.0])\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbafdfed",
      "metadata": {
        "id": "cbafdfed"
      },
      "source": [
        "The shapes of `w` and `x` must match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6cbce88b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.790580Z",
          "iopub.status.busy": "2023-01-19T16:24:56.790363Z",
          "iopub.status.idle": "2023-01-19T16:24:56.793312Z",
          "shell.execute_reply": "2023-01-19T16:24:56.792745Z"
        },
        "id": "6cbce88b"
      },
      "outputs": [],
      "source": [
        "assert len(w) == len(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38d0106",
      "metadata": {
        "id": "b38d0106"
      },
      "source": [
        "### `for` loop approach\n",
        "\n",
        "**Task**: *Write a function that uses a `for` loop* to compute the dot product of `w` and `x`. Name the function `dot_loop`. Check that you get `6.0` for the `w` and `x` provided in the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c6255210",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.796192Z",
          "iopub.status.busy": "2023-01-19T16:24:56.795961Z",
          "iopub.status.idle": "2023-01-19T16:24:56.803346Z",
          "shell.execute_reply": "2023-01-19T16:24:56.802712Z"
        },
        "id": "c6255210",
        "outputId": "831f9492-6372-43e7-a193-b143909acc2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def dot_loop(w, x):\n",
        "    sum = 0\n",
        "    i = 0\n",
        "    while i < len(w):\n",
        "      sum += w[i]*x[i]\n",
        "      i += 1\n",
        "    return sum\n",
        "dot_loop(w, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1013e06f",
      "metadata": {
        "id": "1013e06f"
      },
      "source": [
        "Here are some test cases that `dot_loop` should pass. You don't need to understand how this code works yet, but it would reward some study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "07ebc015",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.806585Z",
          "iopub.status.busy": "2023-01-19T16:24:56.806318Z",
          "iopub.status.idle": "2023-01-19T16:24:56.815200Z",
          "shell.execute_reply": "2023-01-19T16:24:56.814518Z"
        },
        "id": "07ebc015",
        "outputId": "db31ff58-595d-4772-b71f-3c0b46f61b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dot_loop(tensor([0.]), tensor([500.]))\n",
            "Testing dot_loop(tensor([1., 0.]), tensor([50.0000,  0.5000]))\n",
            "Testing dot_loop(tensor([-1.,  1.]), tensor([-1.,  1.]))\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "test_cases = [\n",
        "    ([0.], [500.], 0.0),\n",
        "    ([1., 0.0], [50.0, .5], 50.0),\n",
        "    ([-1.0, 1.0], [-1.0, 1.0], 2.0)\n",
        "]\n",
        "def run_dot_test(f, w, x, prod):\n",
        "    w = torch.tensor(w)\n",
        "    x = torch.tensor(x)\n",
        "    print(f\"Testing {f.__name__}({w}, {x})\")\n",
        "    result = f(w, x)\n",
        "    if not isinstance(result, torch.Tensor):\n",
        "        result = torch.tensor(result)\n",
        "    assert torch.isclose(\n",
        "        result,\n",
        "        torch.tensor(prod)\n",
        "    )\n",
        "\n",
        "def run_dot_tests(f):\n",
        "    for w, x, prod in test_cases:\n",
        "        run_dot_test(f, w, x, prod)\n",
        "    print(\"All tests passed\")\n",
        "run_dot_tests(dot_loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04904d45",
      "metadata": {
        "id": "04904d45"
      },
      "source": [
        "\n",
        "#### Torch Elementwise Operations\n",
        "\n",
        "But that's a lot of typing for a concept that we're going to use very frequently. To shorten it (and make it run way faster too!), we'll start taking advantage of some of Torch's builtin functionality.\n",
        "\n",
        "First, we'll learn about *elementwise operations* (called *pointwise operations* in the [PyTorch docs](https://pytorch.org/docs/stable/torch.html#pointwise-ops)).\n",
        "\n",
        "If you try to `*` two Python lists together, you get a `TypeError` (how do you multiply lists??). But in PyTorch (and NumPy, which it's heavily based on), array operations happen *element-by-element* (sometimes called *elementwise*): to multiply two tensors that have the same shape, multiply each number in the first tensor with the corresponding number of the second tensor. The result is a new tensor of the same shape with all the elementwise products.\n",
        "\n",
        "**Task**: Predict what you'll get from running `w * x`. Then try it and compare with your prediction. (No need to write an explanation here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "15630a32",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.818224Z",
          "iopub.status.busy": "2023-01-19T16:24:56.817985Z",
          "iopub.status.idle": "2023-01-19T16:24:56.821272Z",
          "shell.execute_reply": "2023-01-19T16:24:56.820483Z"
        },
        "id": "15630a32",
        "outputId": "0bfabf6f-891d-4251-d986-3b5ab11cf5bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "\"\"\"\n",
        "I used numPy a lot in Discrete Mathematics, and I remember that multiplying\n",
        "two lists together like this would return a list with the product of the original lists.\n",
        "So I expect that that will happen with PyTorch as well, since PyTorch is apparently based on NumPy\n",
        "\"\"\"\n",
        "\n",
        "# your code here\n",
        "w * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe4f5e7",
      "metadata": {
        "id": "dfe4f5e7"
      },
      "source": [
        "### Torch Reduction Ops\n",
        "\n",
        "Torch also provides [*reduction* methods](https://pytorch.org/docs/stable/torch.html#reduction-ops), so named because they *reduce* the number of elements in a Tensor.\n",
        "\n",
        "One really useful reduction op is `.sum`. (I also frequently use `.mean`, `.max`, and `.argmax`).\n",
        "\n",
        "**Task**: Predict the output of running `x.sum()` Then try it and compare with your prediction.\n",
        "\n",
        "> You can also write that as `torch.sum(x)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ec476029",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.823987Z",
          "iopub.status.busy": "2023-01-19T16:24:56.823763Z",
          "iopub.status.idle": "2023-01-19T16:24:56.826385Z",
          "shell.execute_reply": "2023-01-19T16:24:56.825843Z"
        },
        "id": "ec476029",
        "outputId": "444506cf-3a3c-4b03-e6dc-2ba57f533b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.5000)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "\"\"\"The sum of the numbers in x (1.5 and -3) is -1.5.\n",
        "So that should be the result of x.sum.\"\"\"\n",
        "\n",
        "# your code here\n",
        "x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c60f49e",
      "metadata": {
        "id": "4c60f49e"
      },
      "source": [
        "### Building a dot product out of Torch ops\n",
        "\n",
        "Now **make a new version of `dot_loop`, called `dot_ops`**, that uses an elementwise op to multiply corresponding numbers and a reduction op to sum the result. Check that the result still passes the tests. (*Don't use `@` yet.*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c901a58b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:56.829092Z",
          "iopub.status.busy": "2023-01-19T16:24:56.828874Z",
          "iopub.status.idle": "2023-01-19T16:24:57.026469Z",
          "shell.execute_reply": "2023-01-19T16:24:57.025814Z"
        },
        "id": "c901a58b",
        "outputId": "3140b97d-abf2-4430-d8ce-336101865748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "def dot_ops(w, x):\n",
        "    sum_list = w * x\n",
        "    return sum_list.sum()\n",
        "dot_ops(w, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa79a42b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.029928Z",
          "iopub.status.busy": "2023-01-19T16:24:57.029651Z",
          "iopub.status.idle": "2023-01-19T16:24:57.033550Z",
          "shell.execute_reply": "2023-01-19T16:24:57.032846Z"
        },
        "id": "aa79a42b",
        "outputId": "a27bc652-5ac4-4754-a15e-7b9db01f25d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing dot_ops(tensor([0.]), tensor([500.]))\n",
            "Testing dot_ops(tensor([1., 0.]), tensor([50.0000,  0.5000]))\n",
            "Testing dot_ops(tensor([-1.,  1.]), tensor([-1.,  1.]))\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "run_dot_tests(dot_ops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d123d0a3",
      "metadata": {
        "id": "d123d0a3"
      },
      "source": [
        "\n",
        "Finally, since `dot` is such an important operation, PyTorch provides it directly:\n",
        "\n",
        "```python\n",
        "torch.dot(w, x)\n",
        "```\n",
        "\n",
        "Python recently introduced a \"matmul operator\", `@`, that does the same thing.\n",
        "\n",
        "```python\n",
        "w @ x\n",
        "```\n",
        "\n",
        "To apply this knowledge, let's try writing a slightly more complex function: a linear transformation layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d7ad45",
      "metadata": {
        "id": "45d7ad45"
      },
      "source": [
        "## Linear Layer\n",
        "\n",
        "The most basic component of a neural network (and many other machine learning methods) is a *linear transformation layer*. Going back to our `y = w*x + b` example, the `w*x + b` is the linear transformation: given an `x`, dot it with some `w`eights and add a `b`ias.\n",
        "\n",
        "**Task**: **Write a function that performs a linear transformation of a vector `x`.** Use PyTorch's built-in functionality for dot products (`torch.dot()` or ` @`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a5081a00",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.037326Z",
          "iopub.status.busy": "2023-01-19T16:24:57.037003Z",
          "iopub.status.idle": "2023-01-19T16:24:57.042871Z",
          "shell.execute_reply": "2023-01-19T16:24:57.042174Z"
        },
        "id": "a5081a00",
        "outputId": "69a53641-59b6-47a4-9ee9-c69536370993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def linear(weights, bias, x):\n",
        "    return weights @ x + bias\n",
        "linear(w, -4.0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1688ede2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.046380Z",
          "iopub.status.busy": "2023-01-19T16:24:57.046084Z",
          "iopub.status.idle": "2023-01-19T16:24:57.050544Z",
          "shell.execute_reply": "2023-01-19T16:24:57.049806Z"
        },
        "id": "1688ede2"
      },
      "outputs": [],
      "source": [
        "assert torch.isclose(linear(w, -4.0, x), torch.tensor(2.0))\n",
        "assert torch.isclose(linear(w, 0.0, x), torch.tensor(6.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330a305d",
      "metadata": {
        "id": "330a305d"
      },
      "source": [
        "\n",
        "\n",
        "### Linear layer, Module-style\n",
        "\n",
        "Notice that `linear`'s job is to transform `x`, but it needed 3 parameters, not just 1. It would be convenient to view the `linear` function as simply a function of `x`, with `weights` and `bias` being internal details.\n",
        "\n",
        "One way to do this is to make a `Linear` *class* that has these as parameters.\n",
        "\n",
        "**Task**: Fill in the blanks in the template code to do this. (This is roughly how PyTorch's implementation works)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "13a28c6b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.053896Z",
          "iopub.status.busy": "2023-01-19T16:24:57.053629Z",
          "iopub.status.idle": "2023-01-19T16:24:57.059671Z",
          "shell.execute_reply": "2023-01-19T16:24:57.059001Z"
        },
        "id": "13a28c6b",
        "outputId": "dd98dfe0-d654-442c-fbb3-95cda4487607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "class Linear:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weights @ x + self.bias\n",
        "\n",
        "layer = Linear(weights=w, bias=1.0)\n",
        "layer.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfaabead",
      "metadata": {
        "id": "bfaabead"
      },
      "source": [
        "Note: PyTorch's [`Linear` layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) gives a vector-valued output, so to make the dimensionality work out, it actually computes `x @ weights.T + bias`, where `T` computes the transpose of the array."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9945a8ba",
      "metadata": {
        "id": "9945a8ba"
      },
      "source": [
        "\n",
        "## Mean Squared Error\n",
        "\n",
        "Now let's apply what you just learned about elementwise operations on PyTorch tensors to another very common building block in machine learning: measuring *error*.\n",
        "\n",
        "Once we make some predictions, we usually want to be able to measure how *good* the predictions were. For regression tasks, i.e., tasks where we're predicting *numbers*, one very common measure is the *mean squared error*. Here's an algorithm to compute it:\n",
        "\n",
        "- compute `resid` as true (`y_true`) minus predicted (`y_pred`).\n",
        "- compute `squared_error` by squaring each number in `resid`\n",
        "- compute `mean_squared_error` by taking the `mean` of `squared_error`.\n",
        "\n",
        "> **Technical note**: This process implements the mean squared error *loss function*. That is a function that is given some *true* values (call them $y_1$ through $y_n$) and some *predicted* values (call them $\\hat{y}_1$ through $\\hat{y}_n$) and returns $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2.$$\n",
        "\n",
        "Generally you'd get the predicted values, `y_pred`, by calling a function that implements a model (like `linear.forward()` above. But to focus our attention on the error computation, we've provided sample values for `y_true` and `y_pred` that you can just use as-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "29d721c4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.062967Z",
          "iopub.status.busy": "2023-01-19T16:24:57.062716Z",
          "iopub.status.idle": "2023-01-19T16:24:57.066495Z",
          "shell.execute_reply": "2023-01-19T16:24:57.065778Z"
        },
        "id": "29d721c4"
      },
      "outputs": [],
      "source": [
        "y_true = torch.tensor([3.14, 1.59, 2.65])\n",
        "y_pred = torch.tensor([2.71, 8.28, 1.83])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ba9a5a",
      "metadata": {
        "id": "84ba9a5a"
      },
      "source": [
        "**Task**:\n",
        "\n",
        "1. Implement each line of the above algorithm in PyTorch code.\n",
        "    - Use separate cells so you can check the results along the way. For example, the first cell should have two lines, the first to assign (`resid = ...`) and the second to show the result (`resid`).\n",
        "    - **You should not need to write any loops.**\n",
        "    - Try using both `squared_error.mean()` and `torch.mean(squared_error)`.\n",
        "2. Now, write the entire computation in a single succinct expression (i.e., without having to create intermediate variables for `resid` and `squared_error`). Check that you get the same result.\n",
        "\n",
        "> Notes:\n",
        ">\n",
        "> - Recall that Python's exponentiation operator is `**`.\n",
        "> - PyTorch tensors also have a `.pow()` method. So you can also use `.pow(2)`; you might see this in other people's code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e6d26857",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.070093Z",
          "iopub.status.busy": "2023-01-19T16:24:57.069756Z",
          "iopub.status.idle": "2023-01-19T16:24:57.075437Z",
          "shell.execute_reply": "2023-01-19T16:24:57.074615Z"
        },
        "id": "e6d26857",
        "outputId": "751e0148-8a07-47f1-db7f-9413be386597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.4300, -6.6900,  0.8200])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "resid = y_true - y_pred\n",
        "resid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2cc2b1bf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.078512Z",
          "iopub.status.busy": "2023-01-19T16:24:57.078244Z",
          "iopub.status.idle": "2023-01-19T16:24:57.084537Z",
          "shell.execute_reply": "2023-01-19T16:24:57.083887Z"
        },
        "id": "2cc2b1bf",
        "outputId": "7c70e4a8-cb4e-4584-88dc-54a4f3fefcfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1849, 44.7561,  0.6724])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "squared_error = resid * resid\n",
        "squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "58a5818c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.087575Z",
          "iopub.status.busy": "2023-01-19T16:24:57.087328Z",
          "iopub.status.idle": "2023-01-19T16:24:57.093384Z",
          "shell.execute_reply": "2023-01-19T16:24:57.092778Z"
        },
        "id": "58a5818c",
        "outputId": "39d21a81-a00d-4d6b-c4c7-588eac14ef28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.2045)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# your code here to compute MSE from squared_error\n",
        "MSE = squared_error.mean()\n",
        "MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e2653000",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.096182Z",
          "iopub.status.busy": "2023-01-19T16:24:57.095953Z",
          "iopub.status.idle": "2023-01-19T16:24:57.100309Z",
          "shell.execute_reply": "2023-01-19T16:24:57.099701Z"
        },
        "id": "e2653000",
        "outputId": "f707c36b-964f-4508-ec1d-63fe3c1b9ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.2045)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# your code here to do the whole thing in one line\n",
        "MSE = ((y_true - y_pred).pow(2)).mean()\n",
        "MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71982159",
      "metadata": {
        "id": "71982159"
      },
      "source": [
        "## Multidimensional arrays\n",
        "\n",
        "NumPy / PyTorch arrays can have more than one axis. Think of these like lists of lists (of lists of lists of ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "95f5a3d0",
      "metadata": {
        "id": "95f5a3d0",
        "outputId": "593e10c4-c12d-44e7-94ab-c05acf365944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0290, 0.4019],\n",
              "        [0.2598, 0.3666],\n",
              "        [0.0583, 0.7006],\n",
              "        [0.0518, 0.4681]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "torch.manual_seed(1234)\n",
        "x = torch.rand(4, 2)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd3d58f",
      "metadata": {
        "id": "6cd3d58f"
      },
      "source": [
        "**Task**: Use *indexing* to get out the top-left number, the top-right number, the bottom-left, and the bottom-right. One of them is done for you; study how that works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "58de579e",
      "metadata": {
        "id": "58de579e",
        "outputId": "b9aabb79-5fd1-44a0-d5f4-3d09aead166a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4681)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "bottom_right = x[-1, -1]\n",
        "assert bottom_right == x[3, 1] and bottom_right == x[3][-1]\n",
        "bottom_right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5c9960e6",
      "metadata": {
        "id": "5c9960e6",
        "outputId": "9e25f79e-4467-4c44-a6ed-6157cbc210f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-Left: 0.03, Top-Right: 0.40, Bottom-Left: 0.05\n"
          ]
        }
      ],
      "source": [
        "top_left = x[0, 0]\n",
        "top_right = x[0, -1]\n",
        "bottom_left = x[-1, 0]\n",
        "print(f\"Top-Left: {top_left:.2f}, Top-Right: {top_right:.2f}, Bottom-Left: {bottom_left:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503be827",
      "metadata": {
        "id": "503be827"
      },
      "source": [
        "We can apply a reduction operation \"along\" an axis, e.g.,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "862b8297",
      "metadata": {
        "id": "862b8297",
        "outputId": "7065bbd3-735a-4653-bcae-f4c4d4141cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4309, 0.6265, 0.7589, 0.5199])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "x.sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0baf7dde",
      "metadata": {
        "id": "0baf7dde"
      },
      "source": [
        "*Task*: Is summing on `axis=1` summing each row or summing each column?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277aee1d",
      "metadata": {
        "id": "277aee1d"
      },
      "source": [
        "*Summing along axis 1 will sum each row, creating one column of 4 values.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32cf35f7",
      "metadata": {
        "id": "32cf35f7"
      },
      "source": [
        "There's a general rule for what happens when you reduce along an axis: that axis \"goes away\". To think about that rule and its implications, try the following exercise:\n",
        "\n",
        "**Task**: Predict what the `.shape` of each of the following operations will be. Then try each one and check if you were correct. For example, for the first operation, `z.mean(axis=0)`, the shape should be `(6, 7)`; check that it's true and make sure you can explain why (but you don't have to write up that explanation here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "fd556064",
      "metadata": {
        "id": "fd556064",
        "outputId": "01c78b0b-07e7-4bff-b0e4-899f7bcbbf66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z.shape is torch.Size([5, 6, 7])\n",
            "z.mean(axis=0).shape is torch.Size([6, 7])\n",
            "z.shape is torch.Size([5, 6, 7])\n",
            "z.mean(axis=1).shape is torch.Size([5, 7])\n",
            "z.shape is torch.Size([5, 6, 7])\n",
            "z.mean(axis=2).shape is torch.Size([5, 6])\n",
            "z.shape is torch.Size([5, 6, 7])\n",
            "z.mean(axis=-1).shape is torch.Size([5, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "z = torch.rand(5, 6, 7)\n",
        "\n",
        "# example\n",
        "print(f\"z.shape is {z.shape}\")\n",
        "print(f\"z.mean(axis=0).shape is {z.mean(axis=0).shape}\")\n",
        "\n",
        "# z.mean(axis=1).shape will have a shape of torch.shape(5, 7)\n",
        "print(f\"z.shape is {z.shape}\")\n",
        "print(f\"z.mean(axis=1).shape is {z.mean(axis=1).shape}\")\n",
        "\n",
        "# z.mean(axis=2).shape will have a shape of torch.shape(5, 6)\n",
        "print(f\"z.shape is {z.shape}\")\n",
        "print(f\"z.mean(axis=2).shape is {z.mean(axis=2).shape}\")\n",
        "\n",
        "# z.mean(axis=-1).shape will have a shape of torch.shape(5, 6)\n",
        "print(f\"z.shape is {z.shape}\")\n",
        "print(f\"z.mean(axis=-1).shape is {z.mean(axis=-1).shape}\")\n",
        "\n",
        "# Note: indexing is kind of like a reduction operation\n",
        "z[0]\n",
        "z[1].mean(axis=1).shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede2a19b",
      "metadata": {
        "id": "ede2a19b"
      },
      "source": [
        "Finally, the tensor product is a reduction operation that happens between two arrays / tensors. It reduces \"along\" the middle axis.\n",
        "\n",
        "**Task**: Try to find several different shapes that make the following code succeed. Nothing to \"turn in\" here, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "5079b5cd",
      "metadata": {
        "id": "5079b5cd",
        "outputId": "70a49af2-3181-4b1b-8ff2-368ed2761bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "shape1 = (2, 3, 4) # try to find examples with 1, 2, or 3 different numbers here.\n",
        "shape2 = (4, 2) # try to find examples with 1, 2, or 3 different numbers here.\n",
        "x = torch.rand(shape1)\n",
        "y = torch.rand(shape2)\n",
        "(x @ y).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0906eb6c",
      "metadata": {
        "id": "0906eb6c"
      },
      "source": [
        "## Appendix\n",
        "\n",
        "For comparison and future reference, here's PyTorch's internal implementation of MSE loss. There are two ways to access it: the [functional style](https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "f7cb5f99",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.103378Z",
          "iopub.status.busy": "2023-01-19T16:24:57.103110Z",
          "iopub.status.idle": "2023-01-19T16:24:57.110528Z",
          "shell.execute_reply": "2023-01-19T16:24:57.109994Z"
        },
        "id": "f7cb5f99",
        "outputId": "0b6a784f-0eaa-458f-9182-16a9e8711579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.2045)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "F.mse_loss(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c247fa",
      "metadata": {
        "id": "72c247fa"
      },
      "source": [
        "and the [module style](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "e185b825",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-19T16:24:57.113327Z",
          "iopub.status.busy": "2023-01-19T16:24:57.113097Z",
          "iopub.status.idle": "2023-01-19T16:24:57.117794Z",
          "shell.execute_reply": "2023-01-19T16:24:57.117231Z"
        },
        "id": "e185b825",
        "outputId": "8572b048-3acc-4585-a8ce-7826f1050358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.2045)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "loss_fn(y_pred, y_true)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "name": "u02n1-pytorch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}