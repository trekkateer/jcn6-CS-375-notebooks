{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trekkateer/jcn6-CS-375-notebooks/blob/main/4A-softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qModIKFnlHgt"
      },
      "source": [
        "# Softmax, part 1\n",
        "\n",
        "Task: practice using the `softmax` function.\n",
        "\n",
        "**Why**: The softmax is a building block that is used throughout machine learning, statistics, data modeling, and even statistical physics. This activity is designed to get comfortable with how it works at a high and low level.\n",
        "\n",
        "**Note**: Although \"softmax\" is the conventional name in machine learning, you may also see it called \"soft *arg* max\". The [Wikipedia article](https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1065998663) has a good explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eFcKEgYlHgw"
      },
      "source": [
        "### Course Objectives Addressed\n",
        "\n",
        "This notebook addresses the following CS 375 course objective:\n",
        "\n",
        "- TM-Softmax: \"I can implement softmax and explain its role in classification networks.\"\n",
        "\n",
        "It will also help set you up to make progress towards the following objective in future labs:\n",
        "\n",
        "- OG-LossFunctions: \"I can select and compute appropriate loss functions for regression and classification tasks.\"\n",
        "\n",
        "Understanding softmax — how it converts raw scores into a valid probability distribution, its invariance to shifts, and its numerical pitfalls — is essential preparation for working with cross-entropy loss and classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OAqajd7lHgw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yX213qqslHgx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOk4YgQzlHgx"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBtsBdQblHgy"
      },
      "source": [
        "The following function defines `softmax` by using PyTorch built-in functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "91pQGx6klHgy"
      },
      "outputs": [],
      "source": [
        "def softmax_torch(x):\n",
        "    '''Compute the softmax along the last axis, using PyTorch'''\n",
        "    # axis=-1 means the last axis\n",
        "    # This won't matter in this exercise, but it will matter when we get to batches of data.\n",
        "    return torch.softmax(x, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Wy3J-9lHgy"
      },
      "source": [
        "Let's try the PyTorch softmax on an example tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T87-OEhclHgy",
        "outputId": "c3651fb8-a485-4bfe-c333-31a4870aeda4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0900, 0.2447, 0.6652])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x = tensor([1., 2., 3.])\n",
        "softmax_torch(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBZRwY-QlHgz"
      },
      "source": [
        "2. Fill in the following function to implement softmax yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g7du99W5lHgz"
      },
      "outputs": [],
      "source": [
        "def softmax(xx):\n",
        "    # Exponentiate x so all numbers are positive.\n",
        "    expos = xx.exp()\n",
        "    assert expos.min() >= 0\n",
        "    # Normalize (divide by the sum).\n",
        "    return expos/expos.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLP5iuNllHgz"
      },
      "source": [
        "3. Evaluate `softmax(x)` and verify (visually) that it is close to the `softmax_torch(x)` you evaluated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByoVFlENlHgz",
        "outputId": "a429552c-557b-42d9-b1cd-8348805ef96d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0900, 0.2447, 0.6652])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "softmax(x) # tensor([0.0900, 0.2447, 0.6652])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokro0NWlHg0"
      },
      "source": [
        "4. Evaluate each of the following expressions (to make sure you understand what the input is), then evaluate `softmax_torch(__)` for each of the following expressions. Observe how each output relates to `softmax_torch(x)`. (Is it the same? Is it different? Why?)\n",
        "\n",
        "- `x + 1`\n",
        "- `x - 100`\n",
        "- `x - x.max()`\n",
        "- `x * 0.5`\n",
        "- `x * 3.0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQqxtPSflHg0",
        "outputId": "e0efe5d6-2da9-4a68-fedc-6b1853254b0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2., 3., 4.]),\n",
              " tensor([-99., -98., -97.]),\n",
              " tensor([-2., -1.,  0.]),\n",
              " tensor([0.5000, 1.0000, 1.5000]),\n",
              " tensor([3., 6., 9.]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x + 1, x - 100, x - x.max(), x * 0.5, x * 3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DtdIGKulHg0",
        "outputId": "df9f302a-89bd-4772-86b0-e3df3d200619"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0900, 0.2447, 0.6652]),\n",
              " tensor([0.0900, 0.2447, 0.6652]),\n",
              " tensor([0.0900, 0.2447, 0.6652]),\n",
              " tensor([0.1863, 0.3072, 0.5065]),\n",
              " tensor([0.0024, 0.0473, 0.9503]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "softmax_torch(x + 1), softmax_torch(x - 100), softmax_torch(x - x.max()), softmax_torch(x * 0.5), softmax_torch(x * 3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNx2HpZlHg0"
      },
      "source": [
        "*Same or different as `softmax_torch(x)`? Why?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GMqTillHg0"
      },
      "source": [
        "*The first three expressions are almost the exact same as softmax_torch(x), however, the second two diverge. This appears to be because they break with the simply arithmetic nature of the others.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCvocasplHg1"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sh0ryxKlHg1"
      },
      "source": [
        "1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWpukw8tlHg1"
      },
      "source": [
        "*Yes, the tensors that softmax produces are valid probability distributions, because none are negative and they all sum to one.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3z8rqZlHg1"
      },
      "source": [
        "Consider the following situation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wza6IQqlHg1",
        "outputId": "6385c306-364d-4b54-fce1-26badc2514c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0., -1.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "y2 = tensor([1., 0.,])\n",
        "y3 = y2 - 1\n",
        "y3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5TXhRk-lHg1",
        "outputId": "04d2c012-b45d-4389-f4c5-03e57eb7eff9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "y4 = y2 * 2\n",
        "y4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBtsOs4NlHg1"
      },
      "source": [
        "1. Are `softmax(y2)` and `softmax(y3)` the same or different? How could you tell without having to evaluate them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazxqQJKlHg1"
      },
      "source": [
        "*They are not different, because no multiplication has been done to y2 to produce y3.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le_nRyfwlHg1"
      },
      "source": [
        "2. Are `softmax(y2)` and `softmax(y4)` the same or different? How could you tell without having to evaluate them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nDk407ZlHg1"
      },
      "source": [
        "*They are different, because y2 was multiplied by a factor other than one in order to produce y4.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbnn1jzlHg1"
      },
      "source": [
        "## Optional Extension: Numerical Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSPUIo19lHg1"
      },
      "source": [
        "### Task for Numerical Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIKHUUmMlHg1"
      },
      "source": [
        "5. *Numerical issues*. Assign `x2 = 50 * x`. Try `softmax(x2)` and observe that the result includes the dreaded `nan` -- \"not a number\". Something went wrong. **Evaluate the first mathematical operation in `softmax`** for this particularly problematic input. You should see another kind of abnormal value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrWTaleTlHg2",
        "outputId": "4519cebb-1f51-4aab-f30c-f2c8dec3b3c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., nan, nan])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x2 = 50 * x\n",
        "softmax(x2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUmWLpWplHg2",
        "outputId": "b1c978cc-d5a4-466c-e2ab-7dc49a32e1f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5.1847e+21,        inf,        inf])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here (the first mathematical operation in `softmax`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqR3BqFLlHg2"
      },
      "source": [
        "6. *Fixing numerical issues*. Now try `softmax(x2 - 150.0)`. Observe that you now get valid numbers. Also observe how the constant we subtracted relates to the value of `x2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu8vehj0lHg2",
        "outputId": "409d66a8-03a7-49e8-eb80-2f7cd60de255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACAZdHj1lHg2"
      },
      "source": [
        "7. Copy your `softmax` implementation to a new function, `softmax_stable`, and change it so that it subtracts `xx.max()` from `xx` before exponentiating. (Don't use any in-place operations; just use `xx - xx.max()`) Verify that `softmax_stable(x2)` now works, and obtains the same result as `softmax_torch(x2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIrSGaE4lHg2",
        "outputId": "c07eed26-b344-409b-d3a7-4fb8059ebd5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgnj75a4lHg2",
        "outputId": "bb3a780b-a2cf-483a-9e85-02aaf121437c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax_torch(x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9lkz26lHg2"
      },
      "source": [
        "### Analysis of Numerical Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJdkg9VplHg6"
      },
      "source": [
        "3. Explain why `softmax(x2)` failed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtx14DP5lHg7"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv_OYrCvlHg7"
      },
      "source": [
        "4. Use your observations in \\#1-2 above to explain why `softmax_stable` still gives the correct answer for `x` even though we changed the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOEXeCT9lHg7"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9TAmnB2lHg7"
      },
      "source": [
        "5. Explain why `softmax_stable` doesn't give us infinity or Not A Number anymore for `x2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQgBI-WDlHg7"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2nqUm38lHg7"
      },
      "source": [
        "## Extension *optional*\n",
        "\n",
        "Try to prove your observation in Analysis \\#1 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y10XWEglHg7"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "generative_ai_disabled": true,
      "name": "u04n2-softmax",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}